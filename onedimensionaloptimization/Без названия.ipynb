{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1079e56e",
   "metadata": {},
   "source": [
    "# Методы одномерной оптимизации\n",
    "\n",
    "## Постановка задачи.\n",
    "\n",
    "Пусть дана функция $y=f(x)$ необходимо найти минимум этой функции на заданном отрезке $[a, b]$ (задача максимума решается аналогично). Предполагается, что производная функции либо не существует, либо сложно вычислима, что не позволяет свести задачу к поиску корней производной $f\\prime(x)=0$. Методы заключаются в построении последовательности отрезков $[a_n, b_n]$, стаягивающихся к точке $x^{\\ast} = argmin \\{ f(x):\\: x  \\in [a, b] \\}$.\n",
    "\n",
    "### Метод золотого сечения\n",
    "\n",
    "Определение:\n",
    "Говорят, что точка x осуществляет золотое сечение отрезка [a, b], если\n",
    "$\\frac{b-a}{b-x}=\\frac{b-x}{x-a}=\\phi=\\frac{1+\\sqrt{5} }{2}$\n",
    "\n",
    "В качестве $x_1$ и $x_2$ выберем точку золотого сечения отрезка и симметричную ей. Если $a<x_1<x_2<b$, то при указанном выборе точек получаем, что $x_1$ - точка золотого сечения отрезка $[a, x_2]$, а $x_2$ - точка золотого сечения отрезка $[x_1, b]$. Таким образом, на каждом шаге, кроме первого, необходимо вычислять значение только в одной точке, вторая берется из предыдущего шага.\n",
    "Описание метода\n",
    "Параметр на входе:  $\\epsilon$ - достаточно малая положительная константа, погрешность метода.\n",
    "1. $x_1 = b-\\frac{b-a}{\\phi}$, $x_2 = a+\\frac{b-a}{\\phi}$\n",
    "2. Повторять:\n",
    "3. Если $f(x_1) > f(x_2)$, то $a=x_1$, $x_1=x_2$, $x_2=b-(x_1-a)$;\n",
    "4. Если $f(x_1) < f(x_2)$, то $b=x_2$, $x_2=x_1$, $x_1=a+(b-x_2)$;\n",
    "5. пока $\\frac{b-a}{2} \\geq \\epsilon$;\n",
    "6.  $\\tilde{x}^{\\ast}=\\frac{a+b}{2}$.\n",
    "\n",
    "Анализ метода\n",
    "\n",
    "Считаем, что один шаг - это один этап цикла (п. 3-4), $\\lambda=\\frac{1}{\\phi}=\\frac{\\sqrt{5}-1}{2}$. Тогда, считая длину отрезка на каждом шаге $\\Delta_k$ , получаем:\n",
    "$\\Delta_0=a-b$;\n",
    "$\\Delta_1=\\lambda(b-a)=\\lambda\\Delta_0$;\n",
    "$\\Delta_{k+2}=\\Delta_k-\\Delta_{k+1}$;\n",
    "Нетрудно проверить, что\n",
    "(1)\n",
    "$\\Delta_k=\\Delta_k(\\lambda)=(-1)^{k-1}(F_k\\lambda-F_{k-1})\\Delta_0$, где $F_k$-числа Фибоначчи.\n",
    "С другой стороны, выполняется равенство:\n",
    "(2)\n",
    "$\\Delta_k(\\lambda)=\\lambda^k\\Delta_0<0,7^k\\Delta_0$\n",
    "Чтобы погрешность вычисления была менее \\epsilon, должна по крайней мере выполняться оценка на число шагов:\n",
    "$k>\\frac{1}{\\log_{0,7}\\frac{2\\Delta_0}{\\epsilon}}$\n",
    "Тогда значение будет вычисляться в N=k+1точках.\n",
    "\n",
    "### Метод парабол \n",
    "\n",
    "В методе парабол предлагается аппроксимировать оптимизируемую функцию $f(x)$ с помощью квадратичной функции $p(x)=ax^2+bx+c$. Пусть имеются три точки $x_1<x_2<x_3 такие, что интервал $[x_1, x_3] содержит точку минимума функции $f$. Тогда коэффициенты аппроксимирующей параболы $a$,$b$,$c$ могут быть найдены путем решения системы линейных уравнений: $ax^2_i+bx_i+c=f_i=f(x_i), i=1,2,3$. Минимум такой параболы равен: $u=-\\frac{b}{2a}=x_2 - \\frac{(x_2-x_1)^2(f_2-f_3)-(x_2-x_3)^2(f_2-f_1)}{2[(x_2-x_1)(f_2-f_3)-(x_2-x_3)(f_2-f_1)]}$. Если $f_2<f_1$ и $f_2<f_3$, то точка $u$ гарантированно попадает в интервал $[x_1, x_3]$. Таким образом, внутри интервала $[x_1, x_3]$ определены две точки $x_2$ и $u$, с помощью сравнения значений функции $f$ в которых можно сократить интервал поиска.\n",
    "\n",
    "### Комбинированный метод Брента\n",
    "\n",
    "Метод золотого сечения представляет собой надежный способ оптимизации, который сходится за гарантированное число итераций, но обладает лишь линейной скоростью сходимости. Метод парабол работает быстрее в малой окрестности оптимального решения, но может работать долго и неустойчиво на начальных стадиях итерационного процесса. Поэтому на практике для решения задачи одномерной оптимизации используется метод Брента, который эффективно комбинирует эти две стратегии. В данном методе на каждой итерации отслеживаются значения в шести точках (не обязательно различных): $a$,$c$,$x$,$w$,$v$,$u$. Точки $a$,$c$ задают текущий интервал поиска решения, x – точка, соответствующая наименьшему значению функции, w – точка, соответветствующая второму снизу значению функции, $v$ – предыдущее значение $w$. В отличие от метода парабол, в методе Брента аппроксимирующая парабола строится с помощью трех наилучших точек $x$, $w$, $v$ (в случае, если эти три точки различны и значения в них также различны). При этом минимум аппроксимирующей параболы $u$ принимается в качестве следующей точки оптимизационного процесса, если: \n",
    "\n",
    "$\\bullet$ $u$ попадает внутрь интервала $[a, c]$ и отстоит от границ интервала не менее, чем на $\\epsilon$\n",
    "\n",
    "$\\bullet$ если точка $u$ отвергается, то следующая точка находится с помощью золотого сечения большего из интервалов $[a, x]$ и $[x, c]$.\n",
    "\n",
    "Приведем некоторые аргументы в пользу обозначенных условий приема минимума параболы $u$. Так как парабола на текущей итерации проводится через точки $x,w,v$, для которых не гарантируются соотношения $v < x < w$ или $w < x < v$, то минимум параболы может оказаться вне интервала $[a,c]$. Ограничение на максимальную удалённость $u$ от $x$ позволяет избежать слишком больших шагов в оптимизации, которые могут соответствовать биениям в методе парабол. Использование в данном ограничении длины предпредыдущего шага, а не предыдущего, является эвристикой, эффективность которой подтверждается в экспериментах на больших базах задач оптимизации. Эта эвристика предлагает не штрафовать метод за текущий не слишком удачный маленький шаг в надежде на успешные шаги метода на следующих итерациях.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d655f59",
   "metadata": {},
   "source": [
    "### Алгоритм Бройдена — Флетчера — Гольдфарба — Шанно\n",
    "\n",
    "\n",
    "Метод Бройдена — Флетчера — Гольдфарба — Шанно относится к классу так называемых квазиньютоновских методов. В отличие от ньютоновских методов в квазиньютоновских не вычисляется напрямую гессиан функции, т.е. нет необходимости находить частные производные второго порядка. Вместо этого гессиан вычисляется приближенно, исходя из сделанных до этого шагов.\n",
    "\n",
    "\n",
    "Пусть задана некоторая функция $f(x, y)$ и мы решаем задачу оптимизации: $min f(x, y)$.\n",
    "Где в общем случае $f(x, y)$ является не выпуклой функцией, которая имеет непрерывные вторые производные.\n",
    "\n",
    "Шаг №1\n",
    "Инициализируем начальную точку $x_0$;\n",
    "Задаем точность поиска $ε$ > 0;\n",
    "Определяем начальное приближение $H_0 = B_0^{-1}$, где $B_0^{-1}$ — обратный гессиан функции;\n",
    "\n",
    "В качестве начального приближения можно взять гессиан функции, вычисленный в начальной точке $x_0$. Иначе можно использовать хорошо обусловленную, невырожденную матрицу, на практике часто берут единичную матрицу.\n",
    "\n",
    "\n",
    "Шаг №2\n",
    "Находим точку, в направлении которой будем производить поиск, она определяется следующим образом:\n",
    "$p_k = -H_k* \\nabla f_k$\n",
    "\n",
    "\n",
    "\n",
    "Шаг №3\n",
    "Вычисляем $x_{k+1}$ через рекуррентное соотношение:\n",
    "$x_{k+1} = x_k + α_k * p_k$\n",
    "\n",
    "\n",
    "Коэффициент $α_k$ находим используя линейный поиск (line search), где $α_k$ удовлетворяет условиям Вольфе (Wolfe conditions):\n",
    "$f(x_k + α_k * p_k) \\leq f(x_k) + c_1 * α_k * \\nabla f_k^T *p_k$\n",
    "\n",
    "\n",
    "$\\nabla f(x_k + α_k * p_k)^T * p_k \\geq c_2 * \\nabla f_k^T * p_k$\n",
    "\n",
    "\n",
    "Константы $с_1$ и $с_2$ выбирают следующим образом: $0 \\leq c_1 \\leq c_2 \\leq 1$. В большинстве реализаций: $c_1 = 0.0001$ и $с_2 = 0.9$.\n",
    "\n",
    "Фактически мы находим такое $α_k$ при котором значение функции $f(x_k + α_k * p_k)$ минимально.\n",
    "\n",
    "Шаг №4\n",
    "Определяем вектора:\n",
    "$s_k = x_{k+1} - x_k$\n",
    "\n",
    "\n",
    "$y_k = \\nabla f_{k+1} - \\nabla f_k$\n",
    "\n",
    "\n",
    "$s_k$ — шаг алгоритма на итерации, $y_k$ — изменение градиента на итерации.\n",
    "\n",
    "\n",
    "Шаг №5\n",
    "Обновляем гессиан функции, согласно следующей формуле:\n",
    "$H_{k+1} = (I - ρ_k * s_k * y_k^T)H_k(I - ρ_k * y_k * s_k^T) + ρ * s_k * s_k^T$\n",
    "\n",
    "\n",
    "где $ρ_k$\n",
    "$ρ_k = \\frac {1}{y_k^T s_k}$\n",
    "\n",
    "\n",
    "$I$ — единичная матрица.\n",
    "\n",
    "Замечание\n",
    "\n",
    "Выражение вида $y_k * s_k^T$ является внешним произведением (outer product) двух векторов.\n",
    "Пусть определены два вектора $U$ и $V$, тогда их внешнее произведение эквивалентно матричному произведению $UV^T$. Например, для векторов на плоскости:\n",
    "\n",
    "$UV^T = \\begin{pmatrix} U_1 \\\\ U_2 \\end{pmatrix}\\begin{pmatrix} V_1 & V_2 \\end{pmatrix} = \\begin{pmatrix} U_1V_1 & U_1V_2 \\\\ U_2V_1 & U_2V_2 \\end{pmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "Шаг №6\n",
    "Алгоритм продолжает выполнятся до тех пор пока истинно неравенство: $|\\nabla f_k| > ε$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Методы одномерной оптимизации

## Постановка задачи.

Пусть дана функция $y=f(x)$ необходимо найти минимум этой функции на заданном отрезке $[a, b]$ (задача максимума решается аналогично). Предполагается, что производная функции либо не существует, либо сложно вычислима, что не позволяет свести задачу к поиску корней производной $f\prime(x)=0$. Методы заключаются в построении последовательности отрезков [a_n, b_n], стаягивающихся к точке $x^{\ast} = argmin \{ f(x):\: x  \in [a, b] \}$.

### Метод золотого сечения

Определение:
Говорят, что точка x осуществляет золотое сечение отрезка [a, b], если
$\frac{b-a}{b-x}=\frac{b-x}{x-a}=\phi=\frac{1+\sqrt{5} }{2}$

В качестве $x_1$ и $x_2$ выберем точку золотого сечения отрезка и симметричную ей. Если $a<x_1<x_2<b$, то при указанном выборе точек получаем, что $x_1$ - точка золотого сечения отрезка $[a, x_2]$, а $x_2$ - точка золотого сечения отрезка $[x_1, b]$. Таким образом, на каждом шаге, кроме первого, необходимо вычислять значение только в одной точке, вторая берется из предыдущего шага.
Описание метода
Параметр на входе:  $\epsilon$ - достаточно малая положительная константа, погрешность метода.
1. $x_1 = b-\frac{b-a}{\phi}$, $x_2 = a+\frac{b-a}{\phi}$
2. Повторять:
3. Если $f(x_1) > f(x_2)$, то $a=x_1$, $x_1=x_2$, $x_2=b-(x_1-a)$;
4. Если $f(x_1) < f(x_2)$, то $b=x_2$, $x_2=x_1$, $x_1=a+(b-x_2)$;
5. пока $\frac{b-a}{2} \geq \epsilon$;
6.  $\tilde{x}^{\ast}=\frac{a+b}{2}$.

Анализ метода

Считаем, что один шаг - это один этап цикла (п. 3-4), $\lambda=\frac{1}{\phi}=\frac{\sqrt{5}-1}{2}$. Тогда, считая длину отрезка на каждом шаге $\Delta_k$ , получаем:
$\Delta_0=a-b$;
$\Delta_1=\lambda(b-a)=\lambda\Delta_0$;
$\Delta_{k+2}=\Delta_k-\Delta_{k+1}$;
Нетрудно проверить, что
(1)
$\Delta_k=\Delta_k(\lambda)=(-1)^{k-1}(F_k\lambda-F_{k-1})\Delta_0$, где $F_k$-числа Фибоначчи.
С другой стороны, выполняется равенство:
(2)
$\Delta_k(\lambda)=\lambda^k\Delta_0<0,7^k\Delta_0$
Чтобы погрешность вычисления была менее \epsilon, должна по крайней мере выполняться оценка на число шагов:
$k>\frac{1}{\log_{0,7}\frac{2\Delta_0}{\epsilon}}$
Тогда значение будет вычисляться в N=k+1точках.

### Метод парабол 

В методе парабол предлагается аппроксимировать оптимизируемую функцию $f(x)$ с помощью квадратичной функции $p(x)=ax^2+bx+c$. Пусть имеются три точки $x_1<x_2<x_3 такие, что интервал $[x_1, x_3] содержит точку минимума функции $f$. Тогда коэффициенты аппроксимирующей параболы $a$,$b$,$c$ могут быть найдены путем решения системы линейных уравнений: $ax^2_i+bx_i+c=f_i=f(x_i), i=1,2,3$. Минимум такой параболы равен: $u=-\frac{b}{2a}=x_2 - \frac{(x_2-x_1)^2(f_2-f_3)-(x_2-x_3)^2(f_2-f_1)}{2[(x_2-x_1)(f_2-f_3)-(x_2-x_3)(f_2-f_1)]}$. Если $f_2<f_1$ и $f_2<f_3$, то точка $u$ гарантированно попадает в интервал $[x_1, x_3]$. Таким образом, внутри интервала $[x_1, x_3]$ определены две точки $x_2$ и $u$, с помощью сравнения значений функции $f$ в которых можно сократить интервал поиска.

### Комбинированный метод Брента

Метод золотого сечения представляет собой надежный способ оптимизации, который сходится за гарантированное число итераций, но обладает лишь линейной скоростью сходимости. Метод парабол работает быстрее в малой окрестности оптимального решения, но может работать долго и неустойчиво на начальных стадиях итерационного процесса. Поэтому на практике для решения задачи одномерной оптимизации используется метод Брента, который эффективно комбинирует эти две стратегии. В данном методе на каждой итерации отслеживаются значения в шести точках (не обязательно различных): $a$,$c$,$x$,$w$,$v$,$u$. Точки $a$,$c$ задают текущий интервал поиска решения, x – точка, соответствующая наименьшему значению функции, w – точка, соответветствующая второму снизу значению функции, $v$ – предыдущее значение $w$. В отличие от метода парабол, в методе Брента аппроксимирующая парабола строится с помощью трех наилучших точек $x$, $w$, $v$ (в случае, если эти три точки различны и значения в них также различны). При этом минимум аппроксимирующей параболы $u$ принимается в качестве следующей точки оптимизационного процесса, если: $u$ попадает внутрь интервала $[a, c]$ и отстоит от границ интервала не менее, чем на $\epsilon$. Если точка $u$ отвергается, то следующая точка находится с помощью золотого сечения большего из интервалов $[a, x]$ и $[x, c]$.

